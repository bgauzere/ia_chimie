{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef8926a2-8a49-45a4-811e-451031dd6a7b",
   "metadata": {},
   "source": [
    "# MLP\n",
    "\n",
    "As a first step, we will implement a MultiLayer Perceptron (MLP) to test it on the classic iris dataset. \n",
    "\n",
    "1. Load the iris dataset and split it into train and test sets \n",
    "> Check the previous pratical sessions !\n",
    "\n",
    "2. Built a Multi Layer Perceptron with default parameters and evaluate its accuracy on test set.\n",
    "\n",
    "3. Plot the loss curve by using the `loss_curve_` attribute of `MLPClassifier`\n",
    "\n",
    "4. Change the `max_iter` to 100 and check the curve and the output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fdfb19-04c5-48d7-8b41-15aa924fabc1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "...\n",
    "X_train, X_test, y_train, y_test = ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffcb343-e389-4837-a2e3-ed51d1650b9c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "correction"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "X,y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c496566f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "clf = \n",
    "...\n",
    "print(\"Accuracy:\", clf.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cff683-e677-4685-a206-cb6e9f822da7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "correction"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "clf = MLPClassifier(random_state=42,verbose=True,max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Accuracy:\", clf.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890d3aa8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c647e98-ce95-45e1-8c33-ce9811947b3a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "correction"
    ]
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(clf.loss_curve_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348a65f6",
   "metadata": {},
   "source": [
    "Now we will implement our MLP on a more complex datasets corresponding to images of handwritten digits. A simplified version of this dataset is provided within the `load_digits` function of `sklearn.datasets` module.\n",
    "\n",
    "1. Run a simple MLP classifier on the data. What is the default architecture used by sklearn ?\n",
    "2. Modify your MLP to have 5 hidden layers with following dimensions : \n",
    "    * 32 neurons\n",
    "    * 64 neurons\n",
    "    * 128 neurons\n",
    "    * 64 neurons\n",
    "    * 32 neurons\n",
    "3. What do you observe on learning process and results ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736a9aeb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "X,y = load_digits(return_X_y=True)\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052a1659-b4ad-404f-84d2-a0eab7774917",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "correction"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "X,y = load_digits(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "clf = MLPClassifier(hidden_layer_sizes=[32,64,128,64,32],random_state=42,verbose=True,max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Accuracy:\", clf.score(X_test, y_test))\n",
    "plt.plot(clf.loss_curve_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7756f629-13b6-45f0-8553-cc0ab14b35ae",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# GNN\n",
    "\n",
    "In this second part, we will use a Graph Neural Network (GNN), implemented in its simplest form. \n",
    "All the code is provided since the implementation is a little bit more complex than using `sklearn`. Nonetheless, take time to understand the code and retrieve the components and steps of a Graph Neural Network.\n",
    "\n",
    "To make it work, install `torch` (check command in Readme.md) and copy/paste the `greycdata` folder available on Universitice in the same folder as notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144b6bbf-c684-4a0a-80bb-3d6d34fac5e5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "import torch\n",
    "from greycdata.datasets import GreycDataset\n",
    "\n",
    "# Loading the Acyclic dataset\n",
    "dataset = GreycDataset(name='Acyclic',root='data/Acyclic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24c6db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "dataset = dataset.shuffle()\n",
    "ratio_train = .9\n",
    "size_train = int(len(dataset)*ratio_train)\n",
    "size_test = len(dataset)-size_train\n",
    "train_dataset = dataset[:size_train]\n",
    "test_dataset = dataset[size_train:]\n",
    "\n",
    "print(f'Number of training graphs: {len(train_dataset)}')\n",
    "print(f'Number of test graphs: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95168198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to torch data\n",
    "from torch_geometric.loader import DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcc7cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import global_add_pool\n",
    "\n",
    "# Creation of a basic GCN model. \n",
    "class MyGCN(torch.nn.Module):\n",
    "    def __init__(self, input_channels,hidden_channels):\n",
    "        super(MyGCN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.lin = Linear(hidden_channels, 1)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # Convolution layers\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        \n",
    "        # Read out (pooling) layer\n",
    "        x = global_add_pool(x, batch) \n",
    "        x = self.lin(x)\n",
    "        return x\n",
    "\n",
    "model = MyGCN(input_channels=dataset.num_features,hidden_channels=64)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bcf1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyGCN(input_channels=dataset.num_features,hidden_channels=128)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01) # How we make the gradient descent\n",
    "criterion = torch.nn.MSELoss(reduction=\"sum\") # How we compute the loss, evaluate the performance of the model on train set\n",
    "\n",
    "def my_mse(gt,pred):\n",
    "    \"\"\"\n",
    "    Compute the sum of squared errors between gt and pred\n",
    "    \"\"\"\n",
    "    return ((gt-pred)**2).sum()\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    loss_epoch = 0.0\n",
    "    for data in train_loader:  # Iterate in batches over the training dataset.\n",
    "        out = model(data.x, data.edge_index, data.batch)  # Perform a single forward pass.\n",
    "        loss = criterion(out, data.y.reshape(-1,1))  # Compute the loss.\n",
    "        loss_epoch += loss.item()\n",
    "        loss.backward()  # Derive gradients.\n",
    "        optimizer.step()  # Update parameters based on gradients.\n",
    "        optimizer.zero_grad()  # Clear gradients.\n",
    "        loss_epoch += loss.item()\n",
    "    return loss_epoch\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    sse = 0.0\n",
    "    nb = 0\n",
    "    for data in loader:  # Iterate in batches over the training/test dataset.\n",
    "        out = model(data.x, data.edge_index, data.batch)  \n",
    "        sse += my_mse(out, data.y.reshape(-1,1))\n",
    "    return sse\n",
    "\n",
    "losses=[]\n",
    "\n",
    "for epoch in tqdm(range(1, 100)):\n",
    "    loss = train()\n",
    "    losses.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef2d669",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ia_pour_la_chimie",
   "language": "python",
   "name": "ia_pour_la_chimie"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
